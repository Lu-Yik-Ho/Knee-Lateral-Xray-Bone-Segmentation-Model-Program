{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b19c3f0",
   "metadata": {},
   "source": [
    "**Bone Segmentation Model for Knee Joint Lateral X-Ray (Main Jupyter Notebook)**\n",
    "\n",
    "\n",
    "Version 1\n",
    "\n",
    "By Lu Yik Ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747760ae",
   "metadata": {},
   "source": [
    "**Step 1: Program Requirements**\n",
    "\n",
    "This program requires preinstalled modules. This step provides a guide on the required modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2008cc5",
   "metadata": {},
   "source": [
    "**Step 1.3: Required Python Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a86416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules.\n",
    "\n",
    "# Run this code box once to ensure all modules are installed.\n",
    "\n",
    "import torch\n",
    "\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "import pydicom\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import shutil\n",
    "import ultralytics\n",
    "import cv2\n",
    "os.environ['YOLO_VERBOSE'] = 'false'\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "print(\"All modules imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7878910",
   "metadata": {},
   "source": [
    "**Step 2: Preparing Images for LabelMe**\n",
    "\n",
    "These steps retrieve images from DICOM files, cropping said images into squares and applying post-processing to enhance model training.\n",
    "\n",
    "These steps are imported directly from the KneeDetectionLateral project. For more information about these steps, please refer to https://github.com/Lu-Yik-Ho/Knee-Lateral-Xray-RoI-Detection-Model-Program.\n",
    "\n",
    "The training images and inference images should not be split yet to process all images at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b7202",
   "metadata": {},
   "source": [
    "**Step 2.1: Retrieving and Flipping Images from DICOM Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves PNG images from DICOM files\n",
    "\n",
    "from BoneSegmentationLateral_ImageProcessing import dicom_to_png\n",
    "\n",
    "# Input path to DICOM Files\n",
    "dicom_files_lateralL = glob.glob(\"./DICOM Files/**/**/**/LLAT/**\")\n",
    "dicom_files_lateralR = glob.glob(\"./DICOM Files/**/**/**/RLAT/**\")\n",
    "\n",
    "count = 0 \n",
    "\n",
    "# Convert all DICOM files to PNG\n",
    "for file in dicom_files_lateralL:\n",
    "    if not file.endswith(\".png\") and not file.endswith(\".json\") and not file.endswith(\".txt\"):\n",
    "        dicom_to_png(file)\n",
    "        count = count + 1\n",
    "\n",
    "print(\"LL Images: {}\".format(count))\n",
    "count = 0 \n",
    "\n",
    "# Convert all DICOM files to PNG\n",
    "for file in dicom_files_lateralR:\n",
    "    if not file.endswith(\".png\") and not file.endswith(\".json\") and not file.endswith(\".txt\"):\n",
    "        dicom_to_png(file)\n",
    "        count = count + 1\n",
    "\n",
    "print(\"RL Images: {}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flips right knee images horizontally\n",
    "\n",
    "from BoneSegmentationLateral_ImageProcessing import flip_png\n",
    "\n",
    "# Input target PNG files\n",
    "png_files = glob.glob(\"./DICOM Files/**/**/**/RLAT/**.png\")\n",
    "\n",
    "# Flip all PNG files\n",
    "for file in png_files:\n",
    "    \n",
    "    # Prevents flipping twice\n",
    "    if not file.endswith(\"_F.png\") and not file.endswith(\"_N.png\"):\n",
    "      flip_png(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a82bee",
   "metadata": {},
   "source": [
    "**Step 2.2: Cropping Images into Squares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BoneSegmentationLateral_ImageProcessing import predict_img, calculatePredictBox, calculateSquareBox, crop_img\n",
    "\n",
    "# Load target model\n",
    "model = YOLO(\"./SquareCroppingModel.pt\")\n",
    "\n",
    "# Input target PNG files\n",
    "img_files_L = glob.glob(\"./DICOM Files/**/**/**/LLAT/**.png\")\n",
    "img_files_R = glob.glob(\"./DICOM Files/**/**/**/RLAT/**_F.png\")\n",
    "\n",
    "# Input output directory\n",
    "out_dir = \"./Square Images\"\n",
    "\n",
    "# The part of the bounding box to be changed to create a square bounding box. Enter \"None\" to bypass.\n",
    "square_from = \"Bottom\"\n",
    "\n",
    "for file in img_files_L:\n",
    "    if file.endswith(\".png\"):\n",
    "        result = predict_img(model, file)\n",
    "\n",
    "        predict_box = calculatePredictBox(result)\n",
    "\n",
    "        adjust_box = calculateSquareBox(predict_box, square_from)\n",
    "\n",
    "        crop_img(file, out_dir, float(adjust_box[\"x_min\"]), float(adjust_box[\"y_min\"]), float(adjust_box[\"box_width\"]), float(adjust_box[\"box_height\"]))\n",
    "\n",
    "for file in img_files_R:\n",
    "    if file.endswith(\"_F.png\"):\n",
    "        result = predict_img(model, file)\n",
    "\n",
    "        predict_box = calculatePredictBox(result)\n",
    "\n",
    "        adjust_box = calculateSquareBox(predict_box, square_from)\n",
    "\n",
    "        crop_img(file, out_dir, float(adjust_box[\"x_min\"]), float(adjust_box[\"y_min\"]), float(adjust_box[\"box_width\"]), float(adjust_box[\"box_height\"]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365deacf",
   "metadata": {},
   "source": [
    "**Step 2.3: Processing Square Images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise output images\n",
    "\n",
    "from BoneSegmentationLateral_ImageProcessing import normal_img\n",
    "\n",
    "# Input target PNG files\n",
    "img_files = glob.glob(\"./Square Images/**\")\n",
    "\n",
    "# Input output directory\n",
    "output_dir = \"./Normal Images\"\n",
    "\n",
    "for file in img_files:\n",
    "    if file.endswith(\".png\"):\n",
    "      normal_img(file, True, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLAHE Normalisation on output images\n",
    "\n",
    "from BoneSegmentationLateral_ImageProcessing import apply_clahe\n",
    "\n",
    "# Input target PNG files\n",
    "img_files = glob.glob(\"./Normal Images/**\")\n",
    "\n",
    "# Input output directory\n",
    "output_dir = \"./Training Images\"\n",
    "\n",
    "for file in img_files:\n",
    "    if file.endswith(\"_N.png\"):\n",
    "      apply_clahe(file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f379a1",
   "metadata": {},
   "source": [
    "**Step 3: Creating Bounding Boxes with LabelMe**\n",
    "\n",
    "LabelMe is the main program used to create the polygon area of the bones. These steps provide a guide to using LabelMe for this program.\n",
    "\n",
    "This notebook program is not used for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041642c5",
   "metadata": {},
   "source": [
    "**Step 4: Creating YOLO Required Dataset**\n",
    "\n",
    "These steps convert the current images and labels into a dataset required for training a YOLO model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6792f0",
   "metadata": {},
   "source": [
    "**Step 4.2: Creating YOLO Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce804c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the current data from directories into training and testing data, and saves data to YOLO required directory format\n",
    "\n",
    "# Splits each class into its separate image and text file.\n",
    "\n",
    "from BoneSegmentationLateral_TrainYOLOModel import YOLOTTSplit_splitClass\n",
    "\n",
    "# Input and Output Directories\n",
    "json_files = glob.glob(\"./Training Images/**.json\")\n",
    "img_dir = \"./Training Images\"\n",
    "target_dir = \"./YOLO Dataset\"\n",
    "\n",
    "# Names of classes\n",
    "label_names = [\"Patella\", \"Femur\", \"Tibia\"]\n",
    "\n",
    "# Percentage of data to be used at testing\n",
    "test_section = 0.20\n",
    "\n",
    "YOLOTTSplit_splitClass(json_files, img_dir, target_dir, label_names, test_section = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5c6a1",
   "metadata": {},
   "source": [
    "**Step 4.3: Creating YOLO YAML file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c317782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the required .yaml file required for YOLO\n",
    "\n",
    "from BoneSegmentationLateral_TrainYOLOModel import createYaml\n",
    "\n",
    "# Path to YOLO dataset\n",
    "path = \"./YOLO Dataset\"\n",
    "\n",
    "# Names of classes\n",
    "names = [\"Patella\", \"Femur\", \"Tibia\"]\n",
    "\n",
    "# Relative path to validation images\n",
    "train = \"images/train\"\n",
    "\n",
    "# Relative path to validation images\n",
    "val = \"images/val\"\n",
    "\n",
    "# Number of classses\n",
    "nc = 3 \n",
    "\n",
    "createYaml(path, train, val, nc, names, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02643166",
   "metadata": {},
   "source": [
    "**Step 5: Fine-tuning YOLO Model**\n",
    "\n",
    "These steps fine-tune a YOLO11 nano segmentation model, along with a guide to understand the output metrics of the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b0ecb",
   "metadata": {},
   "source": [
    "**Step 5.1 / 5.2: Fine-Tuning New / Pre-Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a YOLO model with the dataset. \n",
    "\n",
    "# Please make sure the .yaml file is directly under this directory!\n",
    "\n",
    "from BoneSegmentationLateral_TrainYOLOModel import trainModel    \n",
    "\n",
    "# The directory to your YOLO dataset\n",
    "dataset_dir = \"./YOLO Dataset\"\n",
    "\n",
    "# Path to your model. Put None if training brand new model\n",
    "model_path = None\n",
    "\n",
    "# Set number of epochs\n",
    "epoch_num = 50\n",
    "\n",
    "# Toggle simplified output\n",
    "simple_output = True\n",
    "\n",
    "# 1 simple output per number of epochs\n",
    "simple_output_per_epoch = 10\n",
    "\n",
    "\n",
    "trainModel(dataset_dir, model_path, epoch_num, simple_output, simple_output_per_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1dc982",
   "metadata": {},
   "source": [
    "**Step 5.3: Interpreting Training Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BoneSegmentationLateral_TrainYOLOModel import extraResultPlots\n",
    "\n",
    "json_file = \"./YOLO Dataset/output_data.json\"\n",
    "\n",
    "extraResultPlots(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b748d",
   "metadata": {},
   "source": [
    "**Step 6: Inference with Tuned Model**\n",
    "\n",
    "These steps run an inference test with completely new images, testing the performance and capabilities of the trained model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff83016",
   "metadata": {},
   "source": [
    "**Step 6.1: Running Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c866b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BoneSegmentationLateral_TrainYOLOModel import segmentation_inference\n",
    "\n",
    "# Load trained model \n",
    "model = YOLO(\"./runs/segment/500FullTrain/weights/best.pt\")\n",
    "\n",
    "# The directory to your images\n",
    "img_dir = \"./Inference Images\"\n",
    "\n",
    "# The directory to your LabelMe JSON files\n",
    "json_dir = \"./Inference Images\"\n",
    "\n",
    "# The directory to output mask images of predictions.\n",
    "mask_output_dir = \"./Inference Masks/Prediction\"\n",
    "\n",
    "# The directory to output mask images of the ground truth.\n",
    "truth_output_dir = \"./Inference Masks/Ground Truth\"\n",
    "\n",
    "# Names of classes\n",
    "label_names = [\"Patella\", \"Femur\", \"Tibia\"]\n",
    "\n",
    "# Length of each square plot in inches\n",
    "plot_size = 10\n",
    "\n",
    "\n",
    "\n",
    "segmentation_inference(model, img_dir, json_dir, mask_output_dir, truth_output_dir, label_names, plot_size = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b57a2",
   "metadata": {},
   "source": [
    "**Step 7: Creating Mask Images with Tuned Model**\n",
    "\n",
    "If inference yielded successful or expected results, the model can be used to generate mask images for each class detected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a53df",
   "metadata": {},
   "source": [
    "**Step 7.1: Creating Mask Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BoneSegmentationLateral_TrainYOLOModel import predict_imgs\n",
    "\n",
    "\n",
    "# Load trained model \n",
    "model = YOLO(\"./runs/segment/DemoModel/weights/best.pt\")\n",
    "\n",
    "# Input target PNG files\n",
    "img_files = glob.glob(\"./Images for Processing/**.png\")\n",
    "\n",
    "# The directory to output mask images of predictions.\n",
    "mask_output_dir = \"./Processing Image Masks\"\n",
    "\n",
    "# Names of classes\n",
    "label_names = [\"Patella\", \"Femur\", \"Tibia\"]\n",
    "\n",
    "predict_imgs(model, img_files, mask_output_dir, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe869e",
   "metadata": {},
   "source": [
    "**Achknowledgement:**\n",
    "\n",
    "@software{yolov8_ultralytics,\n",
    "  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n",
    "  title = {Ultralytics YOLOv8},\n",
    "  version = {8.0.0},\n",
    "  year = {2023},\n",
    "  url = {https://github.com/ultralytics/ultralytics},\n",
    "  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n",
    "  license = {AGPL-3.0}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
